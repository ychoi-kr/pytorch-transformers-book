{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d63c9c-6298-4615-a205-4b9b5590cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def subset_sampler(dataset, classes, max_len):\n",
    "    target_idx = defaultdict(list)\n",
    "    for idx, label in enumerate(dataset.train_labels):\n",
    "        target_idx[int(label)].append(idx)\n",
    "\n",
    "    indices = list(\n",
    "        chain.from_iterable(\n",
    "            [target_idx[idx][:max_len] for idx in range(len(classes))]\n",
    "        )\n",
    "    )\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
    "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
    "\n",
    "classes = train_dataset.classes\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "\n",
    "subset_train_dataset = subset_sampler(\n",
    "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
    ")\n",
    "subset_test_dataset = subset_sampler(\n",
    "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cf39e2-867c-4d98-a460-44f502021bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"google/vit-base-patch16-224-in21k\"\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(\n",
    "            size=(\n",
    "                image_processor.size[\"height\"],\n",
    "                image_processor.size[\"width\"]\n",
    "            )\n",
    "        ),\n",
    "        transforms.Lambda(\n",
    "            lambda x: torch.cat([x, x, x], 0)\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean=image_processor.image_mean,\n",
    "            std=image_processor.image_std\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb15c67-306b-4ffe-bccd-35bb6fdf2d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collator(data, transform):\n",
    "    images, labels = zip(*data)\n",
    "    pixel_values = torch.stack([transform(image) for image in images])\n",
    "    labels = torch.tensor([label for label in labels])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    subset_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collator(x, transform),\n",
    "    drop_last=True\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    subset_test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collator(x, transform),\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e9422-7399-4531-b34d-6d84e1b1d890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SwinForImageClassification\n",
    "\n",
    "\n",
    "model = SwinForImageClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"microsoft/swin-tiny-patch4-window7-224\",\n",
    "    num_labels=len(train_dataset.classes),\n",
    "    id2label={idx: label for label, idx in train_dataset.class_to_idx.items()},\n",
    "    label2id=train_dataset.class_to_idx,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│  └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                if sssub_name == \"projection\":\n",
    "                    print(\"│  │  └\", sssub_name, sssub_module)\n",
    "                else:\n",
    "                    print(\"│  │  └\", sssub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a01960-903b-45f5-9c54-760098f8c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(\"이미지 차원 :\", batch[\"pixel_values\"].shape)\n",
    "\n",
    "patch_emb_output, shape = model.swin.embeddings.patch_embeddings(batch[\"pixel_values\"])\n",
    "print(\"모듈:\", model.swin.embeddings.patch_embeddings)\n",
    "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25955635-41b2-4516-8c5f-7b98fddf9edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for main_name, main_module in model.swin.encoder.layers[0].named_children():\n",
    "    print(main_name) \n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"│ └\", ssub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f270eb2-37a6-49ff-b937-4b1bb9d5496b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.swin.encoder.layers[0].blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21bf119-4613-4a1c-94c9-0fb002b1989c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)\n",
    "\n",
    "W_MSA = model.swin.encoder.layers[0].blocks[0]\n",
    "SW_MSA = model.swin.encoder.layers[0].blocks[1]\n",
    "\n",
    "W_MSA_output = W_MSA(patch_emb_output, W_MSA.input_resolution)[0]\n",
    "SW_MSA_output = SW_MSA(W_MSA_output, SW_MSA.input_resolution)[0]\n",
    "\n",
    "print(\"W-MSA 결과 차원 :\", W_MSA_output.shape)\n",
    "print(\"SW-MSA 결과 차원 :\", SW_MSA_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f0972-23dc-4136-a94c-ec18eb6d3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_merge = model.swin.encoder.layers[0].downsample\n",
    "print(\"patch_merge 모듈 :\", patch_merge)\n",
    "\n",
    "output = patch_merge(SW_MSA_output, patch_merge.input_resolution)\n",
    "print(\"patch_merge 결과 차원 :\", output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

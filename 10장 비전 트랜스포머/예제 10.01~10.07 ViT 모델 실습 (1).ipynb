{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1637c8-d4c0-464d-9f6b-1a8be4b48b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def subset_sampler(dataset, classes, max_len):\n",
    "    target_idx = defaultdict(list)\n",
    "    for idx, label in enumerate(dataset.train_labels):\n",
    "        target_idx[int(label)].append(idx)\n",
    "\n",
    "    indices = list(\n",
    "        chain.from_iterable(\n",
    "            [target_idx[idx][:max_len] for idx in range(len(classes))]\n",
    "        )\n",
    "    )\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
    "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
    "\n",
    "classes = train_dataset.classes\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "\n",
    "print(classes)\n",
    "print(class_to_idx)\n",
    "\n",
    "subset_train_dataset = subset_sampler(\n",
    "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
    ")\n",
    "subset_test_dataset = subset_sampler(\n",
    "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
    ")\n",
    "\n",
    "print(f\"Training Data Size : {len(subset_train_dataset)}\")\n",
    "print(f\"Testing Data Size : {len(subset_test_dataset)}\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43331628-60a7-4640-b547-3badae816264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"google/vit-base-patch16-224-in21k\"\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(\n",
    "            size=(\n",
    "                image_processor.size[\"height\"],\n",
    "                image_processor.size[\"width\"]\n",
    "            )\n",
    "        ),\n",
    "        transforms.Lambda(\n",
    "            lambda x: torch.cat([x, x, x], 0)\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean=image_processor.image_mean,\n",
    "            std=image_processor.image_std\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"size : {image_processor.size}\")\n",
    "print(f\"mean : {image_processor.image_mean}\")\n",
    "print(f\"std : {image_processor.image_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9ad8d-a37a-452c-aee8-0562c982bb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collator(data, transform):\n",
    "    images, labels = zip(*data)\n",
    "    pixel_values = torch.stack([transform(image) for image in images])\n",
    "    labels = torch.tensor([label for label in labels])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    subset_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collator(x, transform),\n",
    "    drop_last=True\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    subset_test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collator(x, transform),\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for key, value in batch.items():\n",
    "    print(f\"{key} : {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e821280f-e4e7-4aa0-9328-d95a1e3cc139",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"google/vit-base-patch16-224-in21k\",\n",
    "    num_labels=len(classes),\n",
    "    id2label={idx: label for label, idx in class_to_idx.items()},\n",
    "    label2id=class_to_idx,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1414f6a-c1be-460a-b6ac-40790faafc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.vit.embeddings)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "print(\"image shape :\", batch[\"pixel_values\"].shape)\n",
    "print(\"patch embeddings shape :\",\n",
    "    model.vit.embeddings.patch_embeddings(batch[\"pixel_values\"]).shape\n",
    ")\n",
    "print(\"[CLS] + patch embeddings shape :\",\n",
    "    model.vit.embeddings(batch[\"pixel_values\"]).shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1b460-47aa-4c5f-9086-aec48a725548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"../models/ViT-FashionMNIST\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.001,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=125,\n",
    "    remove_unused_columns=False,\n",
    "    seed=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83101c79-51f6-4b23-83ef-082e15bdd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"f1\")\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    macro_f1 = metric.compute(\n",
    "        predictions=predictions, references=labels, average=\"macro\"\n",
    "    )\n",
    "    return macro_f1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

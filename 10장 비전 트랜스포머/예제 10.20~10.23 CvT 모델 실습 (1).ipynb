{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e00d6d-5009-4113-b19c-a6aae119bd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import Subset\n",
    "from torchvision import datasets\n",
    "\n",
    "\n",
    "def subset_sampler(dataset, classes, max_len):\n",
    "    target_idx = defaultdict(list)\n",
    "    for idx, label in enumerate(dataset.train_labels):\n",
    "        target_idx[int(label)].append(idx)\n",
    "\n",
    "    indices = list(\n",
    "        chain.from_iterable(\n",
    "            [target_idx[idx][:max_len] for idx in range(len(classes))]\n",
    "        )\n",
    "    )\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "\n",
    "train_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=True)\n",
    "test_dataset = datasets.FashionMNIST(root=\"../datasets\", download=True, train=False)\n",
    "\n",
    "classes = train_dataset.classes\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "\n",
    "subset_train_dataset = subset_sampler(\n",
    "    dataset=train_dataset, classes=train_dataset.classes, max_len=1000\n",
    ")\n",
    "subset_test_dataset = subset_sampler(\n",
    "    dataset=test_dataset, classes=test_dataset.classes, max_len=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787380b7-4525-4a00-b77e-84c9c568cd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"microsoft/cvt-21\"\n",
    ")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(\n",
    "            size=(\n",
    "                image_processor.size[\"shortest_edge\"],\n",
    "                image_processor.size[\"shortest_edge\"]\n",
    "            )\n",
    "        ),\n",
    "        transforms.Lambda(lambda x: torch.cat([x, x, x], 0)),\n",
    "        transforms.Normalize(\n",
    "            mean=image_processor.image_mean,\n",
    "            std=image_processor.image_std\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabbd62a-5ea5-4738-b631-7ef97dfd50d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collator(data, transform):\n",
    "    images, labels = zip(*data)\n",
    "    pixel_values = torch.stack([transform(image) for image in images])\n",
    "    labels = torch.tensor([label for label in labels])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    subset_train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collator(x, transform),\n",
    "    drop_last=True\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    subset_test_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda x: collator(x, transform),\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f969a-c315-4bfc-a2e3-ac64ce375606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CvtForImageClassification\n",
    "\n",
    "\n",
    "model = CvtForImageClassification.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"microsoft/cvt-21\",\n",
    "    num_labels=len(train_dataset.classes),\n",
    "    id2label={idx: label for label, idx in train_dataset.class_to_idx.items()},\n",
    "    label2id=train_dataset.class_to_idx,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "for main_name, main_module in model.named_children():\n",
    "    print(main_name)\n",
    "    for sub_name, sub_module in main_module.named_children():\n",
    "        print(\"└\", sub_name)\n",
    "        for ssub_name, ssub_module in sub_module.named_children():\n",
    "            print(\"   └\", ssub_name)\n",
    "            for sssub_name, sssub_module in ssub_module.named_children():\n",
    "                print(\"     └\", sssub_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669cab46-cf7b-4578-b14f-ab73ecb70de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = model.cvt.encoder.stages\n",
    "print(stages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8a350-6ba4-4508-ab95-1144aa64b748",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "print(\"이미지 차원 :\", batch[\"pixel_values\"].shape)\n",
    "\n",
    "patch_emb_output = stages[0].embedding(batch[\"pixel_values\"])\n",
    "print(\"패치 임베딩 차원 :\", patch_emb_output.shape)\n",
    "\n",
    "batch_size, num_channels, height, width = patch_emb_output.shape\n",
    "hidden_state = patch_emb_output.view(batch_size, num_channels, height * width).permute(0, 2, 1)\n",
    "print(\"셀프 어텐션 입력 차원 :\", hidden_state.shape)\n",
    "\n",
    "attention_output = stages[0].layers[0].attention.attention(hidden_state, height, width)\n",
    "print(\"셀프 어텐션 출력 차원 :\", attention_output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
